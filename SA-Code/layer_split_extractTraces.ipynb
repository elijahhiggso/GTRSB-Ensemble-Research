{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1a8c6027",
      "metadata": {
        "id": "1a8c6027"
      },
      "source": [
        "# Extracting Traces with New Splits and Models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bj4dlftEWmpO",
        "outputId": "3183e24e-190a-4725-a9d5-6ff39b762ba0"
      },
      "id": "bj4dlftEWmpO",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "#/content/drive/MyDrive/split_0.npz\n",
        "\n",
        "npz_file_path = os.path.join('/content/drive/MyDrive', 'test_data.npz')\n",
        "\n",
        "# Check if the file exists\n",
        "if os.path.exists(npz_file_path):\n",
        "else:\n",
        "    print(f\"File does not exist: {npz_file_path}\")\n",
        "\n",
        "# Try to load the .npz file and inspect its content\n",
        "try:\n",
        "    with np.load(npz_file_path) as data:\n",
        "        print(f\"Keys in the file: {data.files}\")\n",
        "\n",
        "        # Print the shape of each array inside the .npz file\n",
        "        for key in data.files:\n",
        "            print(f\"{key} shape: {data[key].shape}\")\n",
        "            print(f\"First element of {key}: {data[key][0]}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6Nzk7nZ43uI",
        "outputId": "89b27331-a012-4c2b-8797-ce23b92e7bd2"
      },
      "id": "K6Nzk7nZ43uI",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File exists: /content/drive/MyDrive/test_data.npz\n",
            "Keys in the file: ['X_test', 'y_test']\n",
            "X_test shape: (7841, 50, 50, 3)\n",
            "First element of X_test: [[[0.14509805 0.14901961 0.14117648]\n",
            "  [0.14509805 0.14901961 0.14117648]\n",
            "  [0.16862746 0.16470589 0.15294118]\n",
            "  ...\n",
            "  [0.16470589 0.16862746 0.15294118]\n",
            "  [0.16078432 0.16470589 0.14901961]\n",
            "  [0.16078432 0.16470589 0.14901961]]\n",
            "\n",
            " [[0.14509805 0.14901961 0.14117648]\n",
            "  [0.14509805 0.14901961 0.14117648]\n",
            "  [0.16862746 0.16470589 0.15294118]\n",
            "  ...\n",
            "  [0.16470589 0.16862746 0.15294118]\n",
            "  [0.16078432 0.16470589 0.14901961]\n",
            "  [0.16078432 0.16470589 0.14901961]]\n",
            "\n",
            " [[0.14901961 0.14901961 0.14117648]\n",
            "  [0.14901961 0.14901961 0.14117648]\n",
            "  [0.15294118 0.15294118 0.14509805]\n",
            "  ...\n",
            "  [0.15294118 0.15294118 0.13725491]\n",
            "  [0.15294118 0.15294118 0.13725491]\n",
            "  [0.15294118 0.15294118 0.13725491]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0.14901961 0.14901961 0.14901961]\n",
            "  [0.14901961 0.14901961 0.14901961]\n",
            "  [0.15686275 0.15686275 0.15686275]\n",
            "  ...\n",
            "  [0.15686275 0.16862746 0.16862746]\n",
            "  [0.14901961 0.14901961 0.14901961]\n",
            "  [0.14901961 0.14901961 0.14901961]]\n",
            "\n",
            " [[0.15294118 0.15294118 0.15294118]\n",
            "  [0.15294118 0.15294118 0.15294118]\n",
            "  [0.15294118 0.15294118 0.15294118]\n",
            "  ...\n",
            "  [0.16862746 0.17254902 0.17254902]\n",
            "  [0.15686275 0.15686275 0.15294118]\n",
            "  [0.15686275 0.15686275 0.15294118]]\n",
            "\n",
            " [[0.15294118 0.15294118 0.15294118]\n",
            "  [0.15294118 0.15294118 0.15294118]\n",
            "  [0.15294118 0.15294118 0.15294118]\n",
            "  ...\n",
            "  [0.16862746 0.17254902 0.17254902]\n",
            "  [0.15686275 0.15686275 0.15294118]\n",
            "  [0.15686275 0.15686275 0.15294118]]]\n",
            "y_test shape: (7841, 43)\n",
            "First element of y_test: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ACTIVATION TRACE EXTRACTION\n",
        "import os\n",
        "import numpy as np\n",
        "import tf_keras as k\n",
        "from multiprocessing import Pool\n",
        "\n",
        "# Constants\n",
        "BATCH_SIZE = 128\n",
        "MODEL_DIR = '/content/drive/MyDrive'  # Adjust to your model directory\n",
        "ACTIVATION_TRACES_DIR = 'activation_traces'\n",
        "PREDICTIONS_DIR = 'predictions'\n",
        "SPLIT_DIR = '/content/drive/MyDrive'  # Adjust based on your data structure\n",
        "os.makedirs(ACTIVATION_TRACES_DIR, exist_ok=True)\n",
        "os.makedirs(PREDICTIONS_DIR, exist_ok=True)\n",
        "\n",
        "# Load test data\n",
        "with np.load(os.path.join(SPLIT_DIR, 'test_data.npz')) as data:\n",
        "    X_test = data['X_test']\n",
        "    y_test = data['y_test']\n",
        "\n",
        "# File paths for saving activation traces and predictions\n",
        "def get_acts_filepath(split, iteration=0):\n",
        "    return os.path.join(ACTIVATION_TRACES_DIR, f\"layer_split_{split}_acts_iter_{iteration}.npy\")\n",
        "\n",
        "def get_preds_filepath(split, iteration=0):\n",
        "    return os.path.join(PREDICTIONS_DIR, f\"layer_split_{split}_preds_iter_{iteration}.npy\")\n",
        "\n",
        "# Extract activation traces with flexibility for dense and convolutional layers\n",
        "def get_act_traces(model, input_set, layer_names, batch_size, num_classes):\n",
        "    temp_model = k.models.Model(inputs=model.input, outputs=[model.get_layer(layer_name).output for layer_name in layer_names])\n",
        "    layer_outputs = temp_model.predict(input_set, batch_size=batch_size, verbose=1)\n",
        "    layer_outputs = layer_outputs if len(layer_names) > 1 else [layer_outputs]\n",
        "\n",
        "    act_traces = None\n",
        "    for layer_name, layer_output in zip(layer_names, layer_outputs):\n",
        "        if layer_output.ndim == 4:  # For convolutional layers\n",
        "            p = Pool(num_classes)\n",
        "            conv_outputs = [layer_output[i] for i in range(len(input_set))]\n",
        "            layer_matrix = np.array(p.map(__aggregate_layer, conv_outputs))\n",
        "            p.join()\n",
        "        elif layer_output.ndim == 2:  # For fully connected (Dense) layers\n",
        "            layer_matrix = np.array(layer_output)\n",
        "        else:\n",
        "            raise Exception(f\"Unsupported output shape found: {layer_output.ndim} for layer {layer_name}\")\n",
        "\n",
        "        act_traces = layer_matrix if act_traces is None else np.append(act_traces, layer_matrix, axis=1)\n",
        "\n",
        "    return act_traces\n",
        "\n",
        "# Aggregate convolutional layer outputs (used if extracting from conv layers)\n",
        "def __aggregate_layer(layer_output):\n",
        "    return [np.mean(layer_output[..., j]) for j in range(layer_output.shape[-1])]\n",
        "\n",
        "\n",
        "def extract_prediction_array(model, input_set, y_true):\n",
        "    # Ensure y_true is in integer label format, not one-hot\n",
        "    y_true = np.argmax(y_true, axis=1)  # Convert from one-hot encoding to integer labels\n",
        "    raw_preds = model.predict(input_set)\n",
        "    pred_labels = np.argmax(raw_preds, axis=1)\n",
        "    pred_confs = np.max(raw_preds, axis=1)\n",
        "    return np.vstack((y_true, pred_labels, pred_confs)).T\n",
        "\n",
        "\n",
        "# Process and save activation traces and predictions\n",
        "def process_and_save(model, split, iteration):\n",
        "    if split == 'test':\n",
        "        X_data = X_test\n",
        "        y_data = y_test\n",
        "    else:\n",
        "        split_file_path = os.path.join(SPLIT_DIR, f'split_{iteration}.npz')\n",
        "        with np.load(split_file_path) as data:\n",
        "            X_data = data[f'X_{split}']\n",
        "            y_data = data[f'y_{split}']\n",
        "\n",
        "    # Set correct layer names depending on the iteration (model)\n",
        "    if iteration == 0:\n",
        "        LAYER_NAMES = ['dense_1'] #dense_1\n",
        "    elif iteration == 1:\n",
        "        LAYER_NAMES = ['dense_3'] #dense_3\n",
        "    elif iteration == 2:\n",
        "        LAYER_NAMES = ['dense_5'] #dense_5\n",
        "\n",
        "    activations = get_act_traces(model, input_set=X_data, layer_names=LAYER_NAMES, batch_size=BATCH_SIZE, num_classes=43)\n",
        "    predictions = extract_prediction_array(model, input_set=X_data, y_true=y_data)\n",
        "    np.save(get_acts_filepath(split, iteration=iteration), activations)\n",
        "    np.save(get_preds_filepath(split, iteration=iteration), predictions)\n",
        "\n",
        "# Iterate through all saved models and process activation traces and predictions\n",
        "for i in range(3):  # Now processing only 3 models/splits\n",
        "    model_path = os.path.join(MODEL_DIR, f'vgg16model_split_{i}.keras')\n",
        "    print(f\"Loading model from {model_path}\")\n",
        "    model = k.models.load_model(model_path)\n",
        "    print(f\"Model {i} loaded successfully\")\n",
        "\n",
        "    # Modify the final dense layer by removing softmax and adding a standalone softmax layer\n",
        "    # First, remove the softmax activation from the final dense layer\n",
        "    model.layers[-1].activation = None  # Deactivate the softmax in the Dense layer\n",
        "\n",
        "    # Then, add a separate softmax layer\n",
        "    from tf_keras.layers import Softmax\n",
        "    model.add(Softmax(name='softmax_output'))\n",
        "\n",
        "    # Print model summary (optional)\n",
        "    model.summary()\n",
        "\n",
        "    for split in ['train', 'test']:  # Process both train and test splits\n",
        "        process_and_save(model, split, iteration=i)\n",
        "        print(f'Processed and saved activations and predictions for model {i} and split {split}')\n",
        "\n",
        "print('Activation trace and prediction extraction complete.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_cHy4hMdeYB",
        "outputId": "1de8848e-f12f-4b37-b400-3e7561adaafd"
      },
      "id": "-_cHy4hMdeYB",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from /content/drive/MyDrive/vgg16model_split_0.keras\n",
            "Model 0 loaded successfully\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " vgg16 (Functional)          (None, 1, 1, 512)         14714688  \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 512)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               262656    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 512)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 43)                22059     \n",
            "                                                                 \n",
            " softmax_output (Softmax)    (None, 43)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14999403 (57.22 MB)\n",
            "Trainable params: 14999403 (57.22 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "197/197 [==============================] - 7s 34ms/step\n",
            "785/785 [==============================] - 10s 12ms/step\n",
            "Processed and saved activations and predictions for model 0 and split train\n",
            "62/62 [==============================] - 3s 44ms/step\n",
            "246/246 [==============================] - 3s 14ms/step\n",
            "Processed and saved activations and predictions for model 0 and split test\n",
            "Loading model from /content/drive/MyDrive/vgg16model_split_1.keras\n",
            "Model 1 loaded successfully\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " vgg16 (Functional)          (None, 1, 1, 512)         14714688  \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 512)               262656    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 43)                22059     \n",
            "                                                                 \n",
            " softmax_output (Softmax)    (None, 43)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14999403 (57.22 MB)\n",
            "Trainable params: 14999403 (57.22 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "197/197 [==============================] - 6s 32ms/step\n",
            "785/785 [==============================] - 9s 12ms/step\n",
            "Processed and saved activations and predictions for model 1 and split train\n",
            "62/62 [==============================] - 2s 34ms/step\n",
            "246/246 [==============================] - 3s 12ms/step\n",
            "Processed and saved activations and predictions for model 1 and split test\n",
            "Loading model from /content/drive/MyDrive/vgg16model_split_2.keras\n",
            "Model 2 loaded successfully\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " vgg16 (Functional)          (None, 1, 1, 512)         14714688  \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 512)               262656    \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 43)                22059     \n",
            "                                                                 \n",
            " softmax_output (Softmax)    (None, 43)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14999403 (57.22 MB)\n",
            "Trainable params: 14999403 (57.22 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "197/197 [==============================] - 7s 33ms/step\n",
            "785/785 [==============================] - 10s 12ms/step\n",
            "Processed and saved activations and predictions for model 2 and split train\n",
            "62/62 [==============================] - 2s 35ms/step\n",
            "246/246 [==============================] - 3s 12ms/step\n",
            "Processed and saved activations and predictions for model 2 and split test\n",
            "Activation trace and prediction extraction complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DEBUG MDSA CALC\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy as sp\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "import tf_keras as k\n",
        "import os\n",
        "\n",
        "# Constants\n",
        "SURPRISE_TYPES = ['MDSA']\n",
        "ACTIVATION_TRACES_DIR = 'activation_traces'\n",
        "PREDICTIONS_DIR = 'predictions'\n",
        "MDSA_SCORES_DIR = 'mdsa_scores'\n",
        "os.makedirs(MDSA_SCORES_DIR, exist_ok=True)\n",
        "\n",
        "def calculate_mdsa(train_acts, train_labels, train_class_count, target_acts, target_preds):\n",
        "    \"\"\"Compute the Mahalanobis Distance-based Surprise Adequacy (MDSA) score.\"\"\"\n",
        "    NEURON_COUNT = len(train_acts[0])\n",
        "    NEURON_COLS = [\"n\" + str(i) for i in range(NEURON_COUNT)]\n",
        "\n",
        "    mdsa_list = []\n",
        "    means_vectors, inv_cov_list = [], []\n",
        "\n",
        "    # Step 1: Calculate mean / inverted covariance matrix for each class\n",
        "    train_df = pd.DataFrame(columns=[\"Ground_Truth\"] + NEURON_COLS)\n",
        "    train_df[\"Ground_Truth\"] = train_labels\n",
        "    for i in range(NEURON_COUNT):\n",
        "        train_df[\"n\" + str(i)] = train_acts[:, i]\n",
        "\n",
        "    for c in range(train_class_count):\n",
        "        subset_df = train_df[train_df[\"Ground_Truth\"] == c]\n",
        "        means_vectors.append(subset_df[NEURON_COLS].mean(axis=0).values)\n",
        "        cov = np.cov(subset_df[NEURON_COLS].values.T)\n",
        "\n",
        "        inv_cov_list.append(sp.linalg.inv(cov))\n",
        "\n",
        "\n",
        "    # Step 2: Calculate MDSA scores\n",
        "    print(\"Calculating Mahalanobis Distance-based Surprise Adequacy scores...\")\n",
        "    for i in tqdm(range(len(target_acts))):\n",
        "        alpha = target_acts[i]\n",
        "        gt_class = target_preds[i].astype(int)\n",
        "\n",
        "        means_vector = means_vectors[gt_class]\n",
        "        inv_cov = inv_cov_list[gt_class]\n",
        "\n",
        "        alpha_means_transpose = np.transpose(alpha - means_vector)\n",
        "\n",
        "        # Calculate the value and print the intermediate steps for debugging\n",
        "        mdsa_value = np.dot(np.dot(alpha_means_transpose, inv_cov), (alpha - means_vector))\n",
        "\n",
        "\n",
        "        # Check if mdsa_value is negative\n",
        "        if mdsa_value < 0:\n",
        "            print(\"Warning: Negative MDSA value detected.\")\n",
        "            print(f\"mdsa_value: {mdsa_value}\")\n",
        "\n",
        "        # Now calculate the MDSA score using the sqrt of the value\n",
        "        mdsa = math.sqrt(mdsa_value)  # This is where the math domain error happens\n",
        "        #mdsa = math.sqrt(max(mdsa_value, 0))\n",
        "\n",
        "        mdsa_list.append(mdsa)\n",
        "\n",
        "    return mdsa_list\n",
        "\n",
        "def get_surprise_scores(surprise_type, train_acts, train_labels, train_class_count, target_acts, target_preds):\n",
        "    if surprise_type == 'MDSA':\n",
        "        return calculate_mdsa(train_acts, train_labels, train_class_count, target_acts, target_preds)\n",
        "    else:\n",
        "        raise Exception(\"Unknown surprise type: \" + str(surprise_type))\n",
        "\n",
        "\n",
        "def extract_parallel_surprise_scores(i: int):\n",
        "    train_acts = np.load(os.path.join(ACTIVATION_TRACES_DIR, f'layer_split_train_acts_iter_{i}.npy'))\n",
        "    train_preds = np.load(os.path.join(PREDICTIONS_DIR, f'layer_split_train_preds_iter_{i}.npy'))\n",
        "\n",
        "    # Access the ground truth labels and predicted labels from columns\n",
        "    train_labels = train_preds[:, 0]  # Ground truth labels are in the first column\n",
        "    train_class_count = len(np.unique(train_labels))\n",
        "\n",
        "    # Process both training and test splits for surprise scores\n",
        "    for data_split in ['train', 'test']:\n",
        "        target_acts = np.load(os.path.join(ACTIVATION_TRACES_DIR, f'layer_split_{data_split}_acts_iter_{i}.npy'))\n",
        "        target_preds_data = np.load(os.path.join(PREDICTIONS_DIR, f'layer_split_{data_split}_preds_iter_{i}.npy'))\n",
        "        target_preds = target_preds_data[:, 1]  # Predicted labels are in the second column\n",
        "\n",
        "        for surprise_type in SURPRISE_TYPES:\n",
        "            scores = get_surprise_scores(surprise_type, train_acts, train_labels, train_class_count, target_acts, target_preds)\n",
        "            output_filepath = os.path.join(MDSA_SCORES_DIR, f'layer_split_{data_split}_mdsa_scores_iter_{i}.npy')\n",
        "            np.save(output_filepath, scores)\n",
        "            print(surprise_type, \"output saved to file:\", output_filepath)\n",
        "\n",
        "\n",
        "# Run MDSA calculation for all models\n",
        "for i in range(3):  # Adjust the range if necessary for the number of splits you have\n",
        "    extract_parallel_surprise_scores(i)\n",
        "\n",
        "print('MDSA score extraction complete.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-HAFSXyeGM4",
        "outputId": "7cdaf4a5-ecd9-494e-8a0d-197aa70f1f0d"
      },
      "id": "D-HAFSXyeGM4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating Mahalanobis Distance-based Surprise Adequacy scores...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25094/25094 [00:00<00:00, 132503.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MDSA output saved to file: mdsa_scores/layer_split_train_mdsa_scores_iter_0.npy\n",
            "Calculating Mahalanobis Distance-based Surprise Adequacy scores...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7841/7841 [00:00<00:00, 118360.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MDSA output saved to file: mdsa_scores/layer_split_test_mdsa_scores_iter_0.npy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating Mahalanobis Distance-based Surprise Adequacy scores...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25094/25094 [00:00<00:00, 131285.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MDSA output saved to file: mdsa_scores/layer_split_train_mdsa_scores_iter_1.npy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating Mahalanobis Distance-based Surprise Adequacy scores...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7841/7841 [00:00<00:00, 132730.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MDSA output saved to file: mdsa_scores/layer_split_test_mdsa_scores_iter_1.npy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating Mahalanobis Distance-based Surprise Adequacy scores...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25094/25094 [00:00<00:00, 132134.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MDSA output saved to file: mdsa_scores/layer_split_train_mdsa_scores_iter_2.npy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating Mahalanobis Distance-based Surprise Adequacy scores...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7841/7841 [00:00<00:00, 129262.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MDSA output saved to file: mdsa_scores/layer_split_test_mdsa_scores_iter_2.npy\n",
            "MDSA score extraction complete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "split = 'train'\n",
        "iteration = 0\n",
        "\n",
        "\n",
        "mdsa_scores_path = f'mdsa_scores/layer_split_{split}_mdsa_scores_iter_{iteration}.npy'\n",
        "mdsa_scores = np.load(mdsa_scores_path)\n",
        "\n",
        "# View the first few MDSA scores\n",
        "print(\"First 10 MDSA scores:\", mdsa_scores[:10])\n",
        "\n",
        "# Check the shape of the MDSA scores\n",
        "print(\"Shape of MDSA scores:\", mdsa_scores.shape)\n",
        "\n",
        "# Summary statistics (optional)\n",
        "print(f\"Mean: {np.mean(mdsa_scores)}\")\n",
        "print(f\"Standard Deviation: {np.std(mdsa_scores)}\")\n",
        "print(f\"Min: {np.min(mdsa_scores)}\")\n",
        "print(f\"Max: {np.max(mdsa_scores)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6EKgGtBPdfW",
        "outputId": "f091c4ee-95d9-4206-be23-8b4ce300ff6c"
      },
      "id": "B6EKgGtBPdfW",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 10 MDSA scores: [ 4.18336998  6.89801051  4.63658183  5.88717019  3.9576132   5.11801757\n",
            " 22.0546794   5.67678852  5.72169485  4.02842221]\n",
            "Shape of MDSA scores: (25094,)\n",
            "Mean: 6.820249849919589\n",
            "Standard Deviation: 16.078938470579008\n",
            "Min: 2.1260977216137364\n",
            "Max: 916.3105087920226\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B56_giI2QTSN"
      },
      "id": "B56_giI2QTSN",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}