{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZ4d-k6tpCF-",
        "outputId": "21f9bd3a-6d27-4329-8988-a27f11b2877c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading gtsrb-german-traffic-sign, 641568792 bytes compressed\n",
            "[==================================================] 641568792 bytes downloaded\n",
            "Downloaded and uncompressed: gtsrb-german-traffic-sign\n",
            "Data source import complete.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'gtsrb-german-traffic-sign:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F82373%2F191501%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20241011%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20241011T184054Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Da92505262752b4d718506025784223d97e2a5c4f7067b1dbb12a0fa0bbf1f28c5d5aa6b05ecfcd48ff7c4c8b85245746157982d08998207b5591a9e6ede5e6ced353869aade0af0e048c6c02053636c7c365db52b77a8acf7a3e3108ccaef6734c50a5487319a5119e74f50a5e04ba88dc62d780c81f29395e988e98052784d78693276024ca87c2c7c5021b8b983a0716ac4b6b37a36e6f3bc81efe18e21c7f599c5e726f7c5c6f1f830669bf6ac1620968d67214d76f3d8ac8075f2efe4c4ee375a95404c92a9d51ef981e265e6ada85a6cdc25aadbf403e552a16f3ccb659be9e64caaf90e10222991029468d4f388a501d3cbaa32e03ef20ae43b44a53ff'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from PIL import Image\n",
        "import os\n",
        "import pathlib\n",
        "import random as rn\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow_hub as hub\n",
        "from tf_keras.preprocessing.image import ImageDataGenerator, img_to_array, array_to_img, load_img\n",
        "from tf_keras.utils import to_categorical\n",
        "from tf_keras.models import Sequential, load_model\n",
        "from tf_keras.applications import VGG19\n",
        "from tf_keras.layers import Conv2D, MaxPool2D, Dense, Flatten, Dropout\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjVV7miYpJkU",
        "outputId": "42813d46-6bc7-4ca2-e669-ffd658e08e71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '../input/gtsrb-german-traffic-sign/'\n",
        "train_path = '../input/gtsrb-german-traffic-sign/Train/'\n",
        "test_path = '../input/gtsrb-german-traffic-sign/Test/'\n",
        "height = 50\n",
        "width = 50"
      ],
      "metadata": {
        "id": "uVSBBkF_p313"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = { 0:'Speed limit (20km/h)',\n",
        "            1:'Speed limit (30km/h)',\n",
        "            2:'Speed limit (50km/h)',\n",
        "            3:'Speed limit (60km/h)',\n",
        "            4:'Speed limit (70km/h)',\n",
        "            5:'Speed limit (80km/h)',\n",
        "            6:'End of speed limit (80km/h)',\n",
        "            7:'Speed limit (100km/h)',\n",
        "            8:'Speed limit (120km/h)',\n",
        "            9:'No passing',\n",
        "            10:'No passing veh over 3.5 tons',\n",
        "            11:'Right-of-way at intersection',\n",
        "            12:'Priority road',\n",
        "            13:'Yield',\n",
        "            14:'Stop',\n",
        "            15:'No vehicles',\n",
        "            16:'Veh > 3.5 tons prohibited',\n",
        "            17:'No entry',\n",
        "            18:'General caution',\n",
        "            19:'Dangerous curve left',\n",
        "            20:'Dangerous curve right',\n",
        "            21:'Double curve',\n",
        "            22:'Bumpy road',\n",
        "            23:'Slippery road',\n",
        "            24:'Road narrows on the right',\n",
        "            25:'Road work',\n",
        "            26:'Traffic signals',\n",
        "            27:'Pedestrians',\n",
        "            28:'Children crossing',\n",
        "            29:'Bicycles crossing',\n",
        "            30:'Beware of ice/snow',\n",
        "            31:'Wild animals crossing',\n",
        "            32:'End speed + passing limits',\n",
        "            33:'Turn right ahead',\n",
        "            34:'Turn left ahead',\n",
        "            35:'Ahead only',\n",
        "            36:'Go straight or right',\n",
        "            37:'Go straight or left',\n",
        "            38:'Keep right',\n",
        "            39:'Keep left',\n",
        "            40:'Roundabout mandatory',\n",
        "            41:'End of no passing',\n",
        "            42:'End no passing veh > 3.5 tons' }"
      ],
      "metadata": {
        "id": "dgtDS_lip_BK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 4\n",
        "seed = 42\n",
        "train_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                   validation_split=0.2)\n",
        "train_dataset = train_datagen.flow_from_directory(train_path,\n",
        "                                                  target_size=(height, width),\n",
        "                                                  batch_size=batch_size,\n",
        "                                                  class_mode='categorical',\n",
        "                                                  shuffle=True,\n",
        "                                                  seed=seed,\n",
        "                                                  color_mode='rgb',\n",
        "                                                  interpolation='hamming',\n",
        "                                                  subset='training')\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                  validation_split=0.2)\n",
        "test_dataset = test_datagen.flow_from_directory(train_path,\n",
        "                                                target_size=(height, width),\n",
        "                                                batch_size=batch_size,\n",
        "                                                class_mode='categorical',\n",
        "                                                shuffle=True,\n",
        "                                                seed=seed,\n",
        "                                                color_mode='rgb',\n",
        "                                                interpolation='hamming',\n",
        "                                                subset='validation')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XRcrrdmqAhi",
        "outputId": "31308c88-eee1-40ea-82fc-47f847a80062"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 31368 images belonging to 43 classes.\n",
            "Found 7841 images belonging to 43 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Conv2D(filters=16, kernel_size=(5,5), activation='relu', input_shape=(height,width,3)),\n",
        "    keras.layers.Conv2D(filters=32, kernel_size=(5,5), activation='relu'),\n",
        "    keras.layers.MaxPool2D(pool_size=(2, 2)),\n",
        "    keras.layers.BatchNormalization(axis=-1),\n",
        "\n",
        "    keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu'),\n",
        "    keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu'),\n",
        "    keras.layers.MaxPool2D(pool_size=(2, 2)),\n",
        "    keras.layers.BatchNormalization(axis=-1),\n",
        "    keras.layers.Dropout(rate=0.25),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(512, activation='relu'),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dropout(rate=0.25),\n",
        "\n",
        "    keras.layers.Dense(43, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GbNbNb-dqLFd",
        "outputId": "ce15c6ce-4598-4290-9420-94b2142db236"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 644
        },
        "id": "DhWHDiujqL4x",
        "outputId": "f3f76baa-a814-48d7-b69c-28ef447d4506"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m46\u001b[0m, \u001b[38;5;34m46\u001b[0m, \u001b[38;5;34m16\u001b[0m)          │           \u001b[38;5;34m1,216\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m42\u001b[0m, \u001b[38;5;34m42\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │          \u001b[38;5;34m12,832\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │             \u001b[38;5;34m128\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m19\u001b[0m, \u001b[38;5;34m19\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │          \u001b[38;5;34m18,496\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │          \u001b[38;5;34m36,928\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_1                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)            │             \u001b[38;5;34m256\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4096\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │       \u001b[38;5;34m2,097,664\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_2                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │           \u001b[38;5;34m2,048\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m43\u001b[0m)                  │          \u001b[38;5;34m22,059\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">46</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">46</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,216</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">42</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">42</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │          <span style=\"color: #00af00; text-decoration-color: #00af00\">12,832</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │          <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │          <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_1                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,097,664</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_2                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">43</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">22,059</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,191,627\u001b[0m (8.36 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,191,627</span> (8.36 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,190,411\u001b[0m (8.36 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,190,411</span> (8.36 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,216\u001b[0m (4.75 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,216</span> (4.75 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "alpha=0.0001\n",
        "epochs=30\n",
        "optim = keras.optimizers.Adam(learning_rate=0.0001)\n",
        "model.compile(optimizer = optim, loss = 'categorical_crossentropy', metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "YTfTqE4RqOvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Extract training data using ImageDataGenerator\n",
        "X_train_list = []\n",
        "y_train_list = []\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    '../input/gtsrb-german-traffic-sign/Train/',\n",
        "    target_size=(50, 50),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Collect all the training data into lists\n",
        "for i in range(len(train_generator)):\n",
        "    X_batch, y_batch = train_generator.__next__()\n",
        "    X_train_list.append(X_batch)\n",
        "    y_train_list.append(y_batch)\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "X_train_full = np.vstack(X_train_list)\n",
        "y_train_full = np.vstack(y_train_list)\n",
        "\n",
        "\n",
        "split_data_dir = 'gtsrb_splits'\n",
        "os.makedirs(split_data_dir, exist_ok=True)\n",
        "\n",
        "## Calculate the exact split ratio to get exactly 7841 test samples\n",
        "split_ratio = 7841 / 39209  # approximately 0.19999\n",
        "\n",
        "## Split the training data\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(X_train_full, y_train_full, test_size=split_ratio, random_state=42)\n",
        "\n",
        "test_file_path = os.path.join(split_data_dir, 'test_data.npz')\n",
        "np.savez(test_file_path, X_test=X_test, y_test=y_test)\n",
        "print(f'Saved derived test set to {test_file_path}')\n",
        "\n",
        "# Gen 3 random splits for training and validation from the remaining 80% training data\n",
        "n_splits = 3\n",
        "for i in range(n_splits):\n",
        "    X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=42+i)\n",
        "\n",
        "    # Save each split\n",
        "    split_file_path = os.path.join(split_data_dir, f'split_{i}.npz')\n",
        "    np.savez(split_file_path, X_train=X_train_split, X_val=X_val_split, y_train=y_train_split, y_val=y_val_split)\n",
        "    print(f'Saved split {i} to {split_file_path}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFNyQYSzqZsj",
        "outputId": "a545579b-2708-4608-fc73-d49252d86d0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 39209 images belonging to 43 classes.\n",
            "Saved derived test set to gtsrb_splits/test_data.npz\n",
            "Saved split 0 to gtsrb_splits/split_0.npz\n",
            "Saved split 1 to gtsrb_splits/split_1.npz\n",
            "Saved split 2 to gtsrb_splits/split_2.npz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the size of the training and test sets after splitting\n",
        "print(f\"Size of training set: {X_train_full.shape[0]}\")\n",
        "print(f\"Size of test set: {X_test.shape[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "maD_lwzkqcrb",
        "outputId": "b2787165-a649-4344-a974-5dd3abeb33ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of training set: 31368\n",
            "Size of test set: 7841\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tf_keras as k\n",
        "from tf_keras.applications import VGG16\n",
        "from tf_keras.models import Sequential\n",
        "from tf_keras.layers import Dense, Dropout, Flatten\n",
        "from tf_keras.optimizers import Adam\n",
        "from tf_keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Directory for the saved splits\n",
        "split_data_dir = 'gtsrb_splits'\n",
        "n_splits = 3  # Number of splits\n",
        "\n",
        "# Train on each split\n",
        "for i in range(n_splits):\n",
        "    # Load the split data\n",
        "    split_file_path = os.path.join(split_data_dir, f'split_{i}.npz')\n",
        "    with np.load(split_file_path) as data:\n",
        "        X_train_split = data['X_train']\n",
        "        y_train_split = data['y_train']\n",
        "        X_val_split = data['X_val']\n",
        "        y_val_split = data['y_val']\n",
        "\n",
        "    # Load the VGG16 base model\n",
        "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=(50, 50, 3))\n",
        "    base_model.summary()\n",
        "    # Unfreeze the last few layers of the base model\n",
        "    for layer in base_model.layers[-5:]:\n",
        "        layer.trainable = True\n",
        "\n",
        "    # Add new layers on top of the base model\n",
        "    model2 = Sequential()\n",
        "    model2.add(base_model)\n",
        "    model2.add(Flatten())\n",
        "    model2.add(Dense(512, activation='relu'))\n",
        "    model2.add(Dropout(0.5))\n",
        "    model2.add(Dense(43, activation='softmax'))  # 43 classes in the GTSRB dataset\n",
        "\n",
        "    # Recreate the optimizer for each model\n",
        "    alpha = 0.0001\n",
        "    optim = Adam(learning_rate=alpha)  # Re-initialize the optimizer\n",
        "\n",
        "    # Compile the model\n",
        "    model2.compile(optimizer=optim, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Set up early stopping and model checkpointing\n",
        "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=5, verbose=1)\n",
        "    checkpoint_filepath = f'best_model_split_{i}.keras'\n",
        "    checkpoint = ModelCheckpoint(checkpoint_filepath, monitor='val_accuracy', verbose=1, save_best_only=True)\n",
        "\n",
        "    # Train the model on the current split\n",
        "    print(f'Training model {i+1} on split {i}...')\n",
        "    model2.fit(\n",
        "        X_train_split, y_train_split,\n",
        "        epochs=10,\n",
        "        validation_data=(X_val_split, y_val_split),\n",
        "        batch_size=32,\n",
        "        callbacks=[early_stopping, checkpoint]\n",
        "    )\n",
        "\n",
        "    # Save the trained model\n",
        "    model_save_path = f'vgg16model_split_{i}.keras'\n",
        "    model2.save(model_save_path)\n",
        "    print(f'Saved model {i+1} to {model_save_path}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTuI43dNqer_",
        "outputId": "fc121d6f-476b-4ea1-aa8f-1a2d776319a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58889256/58889256 [==============================] - 0s 0us/step\n",
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 50, 50, 3)]       0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 50, 50, 64)        1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 50, 50, 64)        36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 25, 25, 64)        0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 25, 25, 128)       73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 25, 25, 128)       147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 12, 12, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 12, 12, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 12, 12, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 12, 12, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 6, 6, 256)         0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 6, 6, 512)         1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 6, 6, 512)         2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 6, 6, 512)         2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 3, 3, 512)         0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 3, 3, 512)         2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 3, 3, 512)         2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 3, 3, 512)         2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 1, 1, 512)         0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14714688 (56.13 MB)\n",
            "Trainable params: 14714688 (56.13 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Training model 1 on split 0...\n",
            "Epoch 1/10\n",
            "785/785 [==============================] - ETA: 0s - loss: 0.8705 - accuracy: 0.7544\n",
            "Epoch 1: val_accuracy improved from -inf to 0.95983, saving model to best_model_split_0.keras\n",
            "785/785 [==============================] - 23s 19ms/step - loss: 0.8705 - accuracy: 0.7544 - val_loss: 0.1290 - val_accuracy: 0.9598\n",
            "Epoch 2/10\n",
            "785/785 [==============================] - ETA: 0s - loss: 0.0961 - accuracy: 0.9763\n",
            "Epoch 2: val_accuracy improved from 0.95983 to 0.98295, saving model to best_model_split_0.keras\n",
            "785/785 [==============================] - 14s 17ms/step - loss: 0.0961 - accuracy: 0.9763 - val_loss: 0.0714 - val_accuracy: 0.9829\n",
            "Epoch 3/10\n",
            "785/785 [==============================] - ETA: 0s - loss: 0.0418 - accuracy: 0.9894\n",
            "Epoch 3: val_accuracy improved from 0.98295 to 0.99602, saving model to best_model_split_0.keras\n",
            "785/785 [==============================] - 14s 18ms/step - loss: 0.0418 - accuracy: 0.9894 - val_loss: 0.0145 - val_accuracy: 0.9960\n",
            "Epoch 4/10\n",
            "785/785 [==============================] - ETA: 0s - loss: 0.0478 - accuracy: 0.9885\n",
            "Epoch 4: val_accuracy did not improve from 0.99602\n",
            "785/785 [==============================] - 13s 17ms/step - loss: 0.0478 - accuracy: 0.9885 - val_loss: 0.0643 - val_accuracy: 0.9845\n",
            "Epoch 5/10\n",
            "785/785 [==============================] - ETA: 0s - loss: 0.0323 - accuracy: 0.9931\n",
            "Epoch 5: val_accuracy did not improve from 0.99602\n",
            "785/785 [==============================] - 13s 17ms/step - loss: 0.0323 - accuracy: 0.9931 - val_loss: 0.0328 - val_accuracy: 0.9922\n",
            "Epoch 6/10\n",
            "785/785 [==============================] - ETA: 0s - loss: 0.0412 - accuracy: 0.9917\n",
            "Epoch 6: val_accuracy did not improve from 0.99602\n",
            "785/785 [==============================] - 13s 17ms/step - loss: 0.0412 - accuracy: 0.9917 - val_loss: 0.0442 - val_accuracy: 0.9900\n",
            "Epoch 7/10\n",
            "785/785 [==============================] - ETA: 0s - loss: 0.0194 - accuracy: 0.9957\n",
            "Epoch 7: val_accuracy did not improve from 0.99602\n",
            "785/785 [==============================] - 13s 17ms/step - loss: 0.0194 - accuracy: 0.9957 - val_loss: 0.0778 - val_accuracy: 0.9812\n",
            "Epoch 8/10\n",
            "785/785 [==============================] - ETA: 0s - loss: 0.0274 - accuracy: 0.9943\n",
            "Epoch 8: val_accuracy did not improve from 0.99602\n",
            "785/785 [==============================] - 13s 17ms/step - loss: 0.0274 - accuracy: 0.9943 - val_loss: 0.0519 - val_accuracy: 0.9925\n",
            "Epoch 8: early stopping\n",
            "Saved model 1 to vgg16model_split_0.keras\n",
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 50, 50, 3)]       0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 50, 50, 64)        1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 50, 50, 64)        36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 25, 25, 64)        0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 25, 25, 128)       73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 25, 25, 128)       147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 12, 12, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 12, 12, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 12, 12, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 12, 12, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 6, 6, 256)         0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 6, 6, 512)         1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 6, 6, 512)         2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 6, 6, 512)         2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 3, 3, 512)         0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 3, 3, 512)         2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 3, 3, 512)         2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 3, 3, 512)         2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 1, 1, 512)         0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14714688 (56.13 MB)\n",
            "Trainable params: 14714688 (56.13 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Training model 2 on split 1...\n",
            "Epoch 1/10\n",
            "785/785 [==============================] - ETA: 0s - loss: 0.7676 - accuracy: 0.7801\n",
            "Epoch 1: val_accuracy improved from -inf to 0.97657, saving model to best_model_split_1.keras\n",
            "785/785 [==============================] - 18s 18ms/step - loss: 0.7676 - accuracy: 0.7801 - val_loss: 0.0840 - val_accuracy: 0.9766\n",
            "Epoch 2/10\n",
            "784/785 [============================>.] - ETA: 0s - loss: 0.1034 - accuracy: 0.9737\n",
            "Epoch 2: val_accuracy improved from 0.97657 to 0.98438, saving model to best_model_split_1.keras\n",
            "785/785 [==============================] - 14s 18ms/step - loss: 0.1034 - accuracy: 0.9737 - val_loss: 0.0604 - val_accuracy: 0.9844\n",
            "Epoch 3/10\n",
            "784/785 [============================>.] - ETA: 0s - loss: 0.0471 - accuracy: 0.9878\n",
            "Epoch 3: val_accuracy did not improve from 0.98438\n",
            "785/785 [==============================] - 13s 17ms/step - loss: 0.0471 - accuracy: 0.9878 - val_loss: 0.0903 - val_accuracy: 0.9750\n",
            "Epoch 4/10\n",
            "785/785 [==============================] - ETA: 0s - loss: 0.0364 - accuracy: 0.9912\n",
            "Epoch 4: val_accuracy improved from 0.98438 to 0.99458, saving model to best_model_split_1.keras\n",
            "785/785 [==============================] - 14s 18ms/step - loss: 0.0364 - accuracy: 0.9912 - val_loss: 0.0207 - val_accuracy: 0.9946\n",
            "Epoch 5/10\n",
            "785/785 [==============================] - ETA: 0s - loss: 0.0437 - accuracy: 0.9901\n",
            "Epoch 5: val_accuracy improved from 0.99458 to 0.99570, saving model to best_model_split_1.keras\n",
            "785/785 [==============================] - 14s 18ms/step - loss: 0.0437 - accuracy: 0.9901 - val_loss: 0.0209 - val_accuracy: 0.9957\n",
            "Epoch 6/10\n",
            "785/785 [==============================] - ETA: 0s - loss: 0.0271 - accuracy: 0.9941\n",
            "Epoch 6: val_accuracy did not improve from 0.99570\n",
            "785/785 [==============================] - 13s 17ms/step - loss: 0.0271 - accuracy: 0.9941 - val_loss: 0.0215 - val_accuracy: 0.9954\n",
            "Epoch 7/10\n",
            "785/785 [==============================] - ETA: 0s - loss: 0.0167 - accuracy: 0.9961\n",
            "Epoch 7: val_accuracy improved from 0.99570 to 0.99602, saving model to best_model_split_1.keras\n",
            "785/785 [==============================] - 14s 18ms/step - loss: 0.0167 - accuracy: 0.9961 - val_loss: 0.0189 - val_accuracy: 0.9960\n",
            "Epoch 8/10\n",
            "785/785 [==============================] - ETA: 0s - loss: 0.0189 - accuracy: 0.9958\n",
            "Epoch 8: val_accuracy improved from 0.99602 to 0.99745, saving model to best_model_split_1.keras\n",
            "785/785 [==============================] - 14s 18ms/step - loss: 0.0189 - accuracy: 0.9958 - val_loss: 0.0098 - val_accuracy: 0.9974\n",
            "Epoch 9/10\n",
            "785/785 [==============================] - ETA: 0s - loss: 0.0316 - accuracy: 0.9936\n",
            "Epoch 9: val_accuracy improved from 0.99745 to 0.99857, saving model to best_model_split_1.keras\n",
            "785/785 [==============================] - 14s 18ms/step - loss: 0.0316 - accuracy: 0.9936 - val_loss: 0.0072 - val_accuracy: 0.9986\n",
            "Epoch 10/10\n",
            "785/785 [==============================] - ETA: 0s - loss: 0.0198 - accuracy: 0.9958\n",
            "Epoch 10: val_accuracy did not improve from 0.99857\n",
            "785/785 [==============================] - 13s 17ms/step - loss: 0.0198 - accuracy: 0.9958 - val_loss: 0.0226 - val_accuracy: 0.9952\n",
            "Saved model 2 to vgg16model_split_1.keras\n",
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 50, 50, 3)]       0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 50, 50, 64)        1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 50, 50, 64)        36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 25, 25, 64)        0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 25, 25, 128)       73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 25, 25, 128)       147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 12, 12, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 12, 12, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 12, 12, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 12, 12, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 6, 6, 256)         0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 6, 6, 512)         1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 6, 6, 512)         2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 6, 6, 512)         2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 3, 3, 512)         0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 3, 3, 512)         2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 3, 3, 512)         2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 3, 3, 512)         2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 1, 1, 512)         0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14714688 (56.13 MB)\n",
            "Trainable params: 14714688 (56.13 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Training model 3 on split 2...\n",
            "Epoch 1/10\n",
            "785/785 [==============================] - ETA: 0s - loss: 0.8587 - accuracy: 0.7546\n",
            "Epoch 1: val_accuracy improved from -inf to 0.96685, saving model to best_model_split_2.keras\n",
            "785/785 [==============================] - 18s 18ms/step - loss: 0.8587 - accuracy: 0.7546 - val_loss: 0.1075 - val_accuracy: 0.9668\n",
            "Epoch 2/10\n",
            "785/785 [==============================] - ETA: 0s - loss: 0.0895 - accuracy: 0.9762\n",
            "Epoch 2: val_accuracy improved from 0.96685 to 0.98693, saving model to best_model_split_2.keras\n",
            "785/785 [==============================] - 14s 18ms/step - loss: 0.0895 - accuracy: 0.9762 - val_loss: 0.0593 - val_accuracy: 0.9869\n",
            "Epoch 3/10\n",
            "785/785 [==============================] - ETA: 0s - loss: 0.0500 - accuracy: 0.9878\n",
            "Epoch 3: val_accuracy did not improve from 0.98693\n",
            "785/785 [==============================] - 13s 17ms/step - loss: 0.0500 - accuracy: 0.9878 - val_loss: 0.1608 - val_accuracy: 0.9648\n",
            "Epoch 4/10\n",
            "785/785 [==============================] - ETA: 0s - loss: 0.0385 - accuracy: 0.9914\n",
            "Epoch 4: val_accuracy did not improve from 0.98693\n",
            "785/785 [==============================] - 13s 17ms/step - loss: 0.0385 - accuracy: 0.9914 - val_loss: 0.0657 - val_accuracy: 0.9852\n",
            "Epoch 5/10\n",
            "785/785 [==============================] - ETA: 0s - loss: 0.0394 - accuracy: 0.9911\n",
            "Epoch 5: val_accuracy did not improve from 0.98693\n",
            "785/785 [==============================] - 13s 17ms/step - loss: 0.0394 - accuracy: 0.9911 - val_loss: 0.0598 - val_accuracy: 0.9863\n",
            "Epoch 6/10\n",
            "785/785 [==============================] - ETA: 0s - loss: 0.0222 - accuracy: 0.9952\n",
            "Epoch 6: val_accuracy improved from 0.98693 to 0.99633, saving model to best_model_split_2.keras\n",
            "785/785 [==============================] - 14s 18ms/step - loss: 0.0222 - accuracy: 0.9952 - val_loss: 0.0161 - val_accuracy: 0.9963\n",
            "Epoch 7/10\n",
            "785/785 [==============================] - ETA: 0s - loss: 0.0208 - accuracy: 0.9956\n",
            "Epoch 7: val_accuracy did not improve from 0.99633\n",
            "785/785 [==============================] - 13s 17ms/step - loss: 0.0208 - accuracy: 0.9956 - val_loss: 0.0411 - val_accuracy: 0.9885\n",
            "Epoch 8/10\n",
            "785/785 [==============================] - ETA: 0s - loss: 0.0315 - accuracy: 0.9933\n",
            "Epoch 8: val_accuracy improved from 0.99633 to 0.99697, saving model to best_model_split_2.keras\n",
            "785/785 [==============================] - 14s 18ms/step - loss: 0.0315 - accuracy: 0.9933 - val_loss: 0.0128 - val_accuracy: 0.9970\n",
            "Epoch 9/10\n",
            "785/785 [==============================] - ETA: 0s - loss: 0.0155 - accuracy: 0.9968\n",
            "Epoch 9: val_accuracy did not improve from 0.99697\n",
            "785/785 [==============================] - 13s 17ms/step - loss: 0.0155 - accuracy: 0.9968 - val_loss: 0.0431 - val_accuracy: 0.9912\n",
            "Epoch 10/10\n",
            "785/785 [==============================] - ETA: 0s - loss: 0.0399 - accuracy: 0.9917\n",
            "Epoch 10: val_accuracy did not improve from 0.99697\n",
            "785/785 [==============================] - 13s 17ms/step - loss: 0.0399 - accuracy: 0.9917 - val_loss: 0.0126 - val_accuracy: 0.9970\n",
            "Saved model 3 to vgg16model_split_2.keras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model2.summary()"
      ],
      "metadata": {
        "id": "NoUC1KOMqhUs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edd92a72-a36c-4e4a-baf8-05ceecb384d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " vgg16 (Functional)          (None, 1, 1, 512)         14714688  \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 512)               262656    \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 43)                22059     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14999403 (57.22 MB)\n",
            "Trainable params: 14999403 (57.22 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    }
  ]
}